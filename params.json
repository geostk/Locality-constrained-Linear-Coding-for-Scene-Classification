{"name":"Locality-constrained-linear-coding-for-scene-classification","tagline":"","body":"\r\n##Project Description\r\n\r\nIn this project, you will implement the LLC (Locality-constrained Linear Coding) method[2] for image classification and apply your method on a natural scene classification dataset.\r\n\r\n## Implementation\r\n\r\n1. Extract SIFT features from each image.\r\n2. Build dictionary use K-means from feature vectors.\r\n3. Encode each feature vector via dictionary using LLC.\r\n4. In Spatial Pyramid Matching part, the pooled features from each sub-region are concatenated and normalized as the final image feature representation. In this paper, we use max pooling combined with sum normalization.\r\n\r\n## DataSet \r\n\r\nThe dataset we used is composed of fifteen scene categories by Svetlana Lazebnik et.al[1].  Each category has 200 to 400 images, and average image size is 300 Ã— 250 pixels. The major sources\r\nof the pictures in the dataset include the COREL collection,\r\npersonal photographs, and Google image search. \r\n\r\n## Evaluation\r\n\r\nWhen evaluate our model, we use a typical training/test split: 100 images per class for training and the rest for testing. \r\n\r\nWe report the confusion matrix for the fifteen scenes categories as well as the mean accuracy.\r\n\r\nIn the mean accuracy measure, we calculate the accuracy of each class, and average the accuracy values of all classes. \r\n\r\n## Spatial Pyramid Matching with Hard Coding\r\n\r\nAfter building dictionary, for each feature vector, we assign it to its closed cluster in dictionary. In other words, each feature vector is assigned to only one cluster. Note that in this experiment, different levels of features are concatenated with different weights. For the features in level k, its weight will be 1/2^(L-k), where L is the number of levels.\r\n\r\n###Parameter Setting\r\n\r\n    params.maxImageSize = 1000;\r\n    params.gridSpacing = 8;\r\n    params.patchSize = 16;\r\n    params.dictionarySize = 200;\r\n    params.numTextonImages = 100;\r\n    params.pyramidLevels = 3;\r\n\r\n### Linear Kernel\r\n\r\nWe first built the model using linear kernel.  The features for each instance is hard-coded word. LibLinear[3] is used to train the model. The results we get is 66.04%.\r\n\r\nNormalized Confusion Matrix\r\n\r\nNote that the y-axis is the true label, the x-axis is the predicted label.\r\n\r\n![](https://github.com/wwu137/Locality-constrained-Linear-Coding-for-Scene-Classification/blob/master/result/cmat_norm_K5_dic2048_l3_s3_C100_B1.jpg?raw=true)\r\n\r\nNon-Normalized Confusion Matrix\r\n\r\n![](https://github.com/wwu137/Locality-constrained-Linear-Coding-for-Scene-Classification/blob/master/result/cmat_nonorm_K5_dic2048_l3_s3_C100_B1.jpg?raw=true)\r\n\r\n### Histogram Intersection Kernel\r\n\r\nWe also built the model using histogram intersection kernel. We first to calculate the kernel for each instance, then train the non-linear SVM model using our kernel. The results we get is 0.8664.\r\n\r\nNormalized Confusion Matrix\r\n\r\nNote that the y-axis is the true label, the x-axis is the predicted label.\r\n\r\n![](https://github.com/wwu137/Locality-constrained-Linear-Coding-for-Scene-Classification/blob/master/result/nonLLC_kernel-3-5-200.jpg?raw=true)\r\n\r\nNon-Normalized Confusion Matrix\r\n\r\n![](https://github.com/wwu137/Locality-constrained-Linear-Coding-for-Scene-Classification/blob/master/result/nonLLC_kernel-3-5-200-Raw.jpg?raw=true)\r\n\r\n\r\n### Summary\r\n\r\nBased on the results, we can see using the histogram intersection kernel does improve our classification model. But due the computation for kernel, the computation complexity is higher than the linear SVM.\r\n\r\n## Locality-constrained Linear Coding\r\n\r\nLLC utilizes the locality constraints to project each descriptor into its local-coordinate system, and the projected coordinates are integrated by max pooling to generate the final representation.\r\n\r\n### Parameter Setting\r\n\r\n\r\n    params.maxImageSize = 1000;\r\n    params.gridSpacing = 8;\r\n    params.patchSize = 16;\r\n    params.dictionarySize = 2048;\r\n    params.numTextonImages = 100;\r\n    params.pyramidLevels = 3;\r\n    params.K = 5;\r\n    liblinear.solver = 3;\r\n    liblinear.C = 100;\r\n    liblinear.B = 1;\r\n    \r\n### Result\r\n\r\nThe mean accuracy we got is 81.14%.\r\n\r\nNormalized Confusion Matrix\r\n\r\nNote that the y-axis is the true label, the x-axis is the predicted label.\r\n\r\n![Confusion Matrix](https://github.com/wwu137/Locality-constrained-Linear-Coding-for-Scene-Classification/blob/master/result/cmat_LLC_norm_K5_dic2048_l3_s3_C100_B1.jpg?raw=true)\r\n\r\nNon-Normalized Confusion Matrix\r\n\r\n![](https://github.com/wwu137/Locality-constrained-Linear-Coding-for-Scene-Classification/blob/master/result/cmat_LLC_norm_K5_dic2048_l3_s3_C100_B1.jpg?raw=true)\r\n\r\n### Grid Search(Bonus)\r\n\r\nWe did extensive grid search for LLC model. The parameters we consider is Solver Type provided by liblinear, Cost for SVM, include bias or not, and dictionary size.\r\n\r\nThe solver types we consider are list as below:\r\n\r\n| Solver                                                        | Index |\r\n|---------------------------------------------------------------|-------|\r\n| L2-regularized logistic regression (primal)                   | 0     |\r\n| L2-regularized L2-loss support vector classification (dual)   | 1     |\r\n| L2-regularized L2-loss support vector classification (primal) | 2     |\r\n| L2-regularized L1-loss support vector classification (dual)   | 3     |\r\n| support vector classification by Crammer and Singer           | 4     |\r\n| L1-regularized L2-loss support vector classification          | 5     |\r\n\r\nFor the cost, we consider five different values: 0.01,0.1,1,10,100.\r\n\r\nWe also consider whether to include the bias or not in the model.\r\n\r\nFinally, we consider the dictionary size as 1024 and 2048.\r\n\r\n| Dictionary Size | Cost | Bias | Solver | Mean Accuracy |\r\n|-----------------|------|------|--------|---------------|\r\n| 1024            | 0.01 | -1   | 0      | 61.95         |\r\n| 1024            | 0.1  | -1   | 0      | 66.95         |\r\n| 1024            | 1    | -1   | 0      | 75.39         |\r\n| 1024            | 10   | -1   | 0      | 79.54         |\r\n| 1024            | 100  | -1   | 0      | 79.8          |\r\n| 1024            | 0.01 | -1   | 1      | 66.35         |\r\n| 1024            | 0.1  | -1   | 1      | 74.67         |\r\n| 1024            | 1    | -1   | 1      | 79.07         |\r\n| 1024            | 10   | -1   | 1      | 79.2          |\r\n| 1024            | 100  | -1   | 1      | 79.08         |\r\n| 1024            | 0.01 | -1   | 2      | 66.39         |\r\n| 1024            | 0.1  | -1   | 2      | 74.67         |\r\n| 1024            | 1    | -1   | 2      | 79.02         |\r\n| 1024            | 10   | -1   | 2      | 79.1          |\r\n| 1024            | 100  | -1   | 2      | 79.21         |\r\n| 1024            | 0.01 | -1   | 3      | 54.81         |\r\n| 1024            | 0.1  | -1   | 3      | 69.04         |\r\n| 1024            | 1    | -1   | 3      | 75.37         |\r\n| 1024            | 10   | -1   | 3      | 79.02         |\r\n| 1024            | 100  | -1   | 3      | 79.02         |\r\n| 1024            | 0.01 | -1   | 4      | 70.7          |\r\n| 1024            | 0.1  | -1   | 4      | 71            |\r\n| 1024            | 1    | -1   | 4      | 79.04         |\r\n| 1024            | 10   | -1   | 4      | 79.88         |\r\n| 1024            | 100  | -1   | 4      | 79.88         |\r\n| 1024            | 0.01 | -1   | 5      | 6.67          |\r\n| 1024            | 0.1  | -1   | 5      | 42.12         |\r\n| 1024            | 1    | -1   | 5      | 63.61         |\r\n| 1024            | 10   | -1   | 5      | 68.71         |\r\n| 1024            | 100  | -1   | 5      | 68.21         |\r\n| 1024            | 0.01 | 1    | 0      | 61.7          |\r\n| 1024            | 0.1  | 1    | 0      | 66.31         |\r\n| 1024            | 1    | 1    | 0      | 75.44         |\r\n| 1024            | 10   | 1    | 0      | 79.3          |\r\n| 1024            | 100  | 1    | 0      | 79.92         |\r\n| 1024            | 0.01 | 1    | 1      | 65.95         |\r\n| 1024            | 0.1  | 1    | 1      | 74.91         |\r\n| 1024            | 1    | 1    | 1      | 79.54         |\r\n| 1024            | 10   | 1    | 1      | 79.73         |\r\n| 1024            | 100  | 1    | 1      | 79.73         |\r\n| 1024            | 0.01 | 1    | 2      | 66.04         |\r\n| 1024            | 0.1  | 1    | 2      | 74.88         |\r\n| 1024            | 1    | 1    | 2      | 79.68         |\r\n| 1024            | 10   | 1    | 2      | 79.63         |\r\n| 1024            | 100  | 1    | 2      | 79.57         |\r\n| 1024            | 0.01 | 1    | 3      | 51            |\r\n| 1024            | 0.1  | 1    | 3      | 70.97         |\r\n| 1024            | 1    | 1    | 3      | 75.46         |\r\n| 1024            | 10   | 1    | 3      | 79.72         |\r\n| 1024            | 100  | 1    | 3      | 79.72         |\r\n| 1024            | 0.01 | 1    | 4      | 64.11         |\r\n| 1024            | 0.1  | 1    | 4      | 71.3          |\r\n| 1024            | 1    | 1    | 4      | 79.98         |\r\n| 1024            | 10   | 1    | 4      | 80.12         |\r\n| 1024            | 100  | 1    | 4      | 80.12         |\r\n| 1024            | 0.01 | 1    | 5      | 6.67          |\r\n| 1024            | 0.1  | 1    | 5      | 10.86         |\r\n| 1024            | 1    | 1    | 5      | 62.6          |\r\n| 1024            | 10   | 1    | 5      | 68.55         |\r\n| 1024            | 100  | 1    | 5      | 67.02         |\r\n| 2048            | 0.01 | -1   | 0      | 61.91         |\r\n| 2048            | 0.1  | -1   | 0      | 67.02         |\r\n| 2048            | 1    | -1   | 0      | 75.55         |\r\n| 2048            | 10   | -1   | 0      | 79.48         |\r\n| 2048            | 100  | -1   | 0      | 80.1          |\r\n| 2048            | 0.01 | -1   | 1      | 66.89         |\r\n| 2048            | 0.1  | -1   | 1      | 75.25         |\r\n| 2048            | 1    | -1   | 1      | 79.64         |\r\n| 2048            | 10   | -1   | 1      | 80            |\r\n| 2048            | 100  | -1   | 1      | 80.14         |\r\n| 2048            | 0.01 | -1   | 2      | 66.85         |\r\n| 2048            | 0.1  | -1   | 2      | 75.18         |\r\n| 2048            | 1    | -1   | 2      | 79.63         |\r\n| 2048            | 10   | -1   | 2      | 80.2          |\r\n| 2048            | 100  | -1   | 2      | 80.25         |\r\n| 2048            | 0.01 | -1   | 3      | 55.27         |\r\n| 2048            | 0.1  | -1   | 3      | 68.96         |\r\n| 2048            | 1    | -1   | 3      | 76.4          |\r\n| 2048            | 10   | -1   | 3      | 80.05         |\r\n| 2048            | 100  | -1   | 3      | 80.05         |\r\n| 2048            | 0.01 | -1   | 4      | 70.64         |\r\n| 2048            | 0.1  | -1   | 4      | 70.75         |\r\n| 2048            | 1    | -1   | 4      | 80.3          |\r\n| 2048            | 10   | -1   | 4      | 80.71         |\r\n| 2048            | 100  | -1   | 4      | 80.71         |\r\n| 2048            | 0.01 | -1   | 5      | 6.67          |\r\n| 2048            | 0.1  | -1   | 5      | 47.77         |\r\n| 2048            | 1    | -1   | 5      | 62.87         |\r\n| 2048            | 10   | -1   | 5      | 68.19         |\r\n| 2048            | 100  | -1   | 5      | 67.34         |\r\n| 2048            | 0.01 | 1    | 0      | 61.92         |\r\n| 2048            | 0.1  | 1    | 0      | 66.56         |\r\n| 2048            | 1    | 1    | 0      | 75.47         |\r\n| 2048            | 10   | 1    | 0      | 79.71         |\r\n| 2048            | 100  | 1    | 0      | 80.58         |\r\n| 2048            | 0.01 | 1    | 1      | 66.19         |\r\n| 2048            | 0.1  | 1    | 1      | 75.15         |\r\n| 2048            | 1    | 1    | 1      | 80.13         |\r\n| 2048            | 10   | 1    | 1      | 80.86         |\r\n| 2048            | 100  | 1    | 1      | 80.87         |\r\n| 2048            | 0.01 | 1    | 2      | 66.3          |\r\n| 2048            | 0.1  | 1    | 2      | 75.22         |\r\n| 2048            | 1    | 1    | 2      | 80.04         |\r\n| 2048            | 10   | 1    | 2      | 80.8          |\r\n| 2048            | 100  | 1    | 2      | 80.42         |\r\n| 2048            | 0.01 | 1    | 3      | 57.78         |\r\n| 2048            | 0.1  | 1    | 3      | 71.06         |\r\n| 2048            | 1    | 1    | 3      | 76.84         |\r\n| 2048            | 10   | 1    | 3      | 81.14         |\r\n| 2048            | 100  | 1    | 3      | 81.14         |\r\n| 2048            | 0.01 | 1    | 4      | 68.91         |\r\n| 2048            | 0.1  | 1    | 4      | 70.15         |\r\n| 2048            | 1    | 1    | 4      | 80.27         |\r\n| 2048            | 10   | 1    | 4      | 80.66         |\r\n| 2048            | 100  | 1    | 4      | 80.66         |\r\n| 2048            | 0.01 | 1    | 5      | 6.67          |\r\n| 2048            | 0.1  | 1    | 5      | 6.67          |\r\n| 2048            | 1    | 1    | 5      | 60.59         |\r\n| 2048            | 10   | 1    | 5      | 68.96         |\r\n| 2048            | 100  | 1    | 5      | 66.63         |\r\n\r\nBased on these results, we can see that larger dictionary size will improve the performance. Include the bias term in SVM model can also increased the performance by around 1% in most cases. Larger cost can improve the performance significantly. The mean accuracy of model with cost as 100 can increased by 20%-60% compared with cost as 0.01.\r\n\r\n\r\n## Reference\r\n\r\n1. Lazebnik, S., Schmid, C., & Ponce, J. (2006). *Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories*. In Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on (Vol. 2, pp. 2169-2178). IEEE.\r\n2. Wang, J., Yang, J., Yu, K., Lv, F., Huang, T., & Gong, Y. (2010, June). *Locality-constrained linear coding for image classification*. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on (pp. 3360-3367). IEEE.\r\n3. Fan, R. E., Chang, K. W., Hsieh, C. J., Wang, X. R., & Lin, C. J. (2008). *LIBLINEAR: A library for large linear classification*. The Journal of Machine Learning Research, 9, 1871-1874.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}