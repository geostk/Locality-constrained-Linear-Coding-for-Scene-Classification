<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Locality-constrained-linear-coding-for-scene-classification : ">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Locality-constrained-linear-coding-for-scene-classification</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/wwu137/Locality-constrained-Linear-Coding-for-Scene-Classification">View on GitHub</a>

          <h1 id="project_title">Locality-constrained-linear-coding-for-scene-classification</h1>
          <h2 id="project_tagline"></h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/wwu137/Locality-constrained-Linear-Coding-for-Scene-Classification/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/wwu137/Locality-constrained-Linear-Coding-for-Scene-Classification/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h2>
<a id="project-description" class="anchor" href="#project-description" aria-hidden="true"><span class="octicon octicon-link"></span></a>Project Description</h2>

<p>In this project, you will implement the LLC (Locality-constrained Linear Coding) method[2] for image classification and apply your method on a natural scene classification dataset.</p>

<h2>
<a id="implementation" class="anchor" href="#implementation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Implementation</h2>

<ol>
<li>Extract SIFT features from each image.</li>
<li>Build dictionary use K-means from feature vectors.</li>
<li>Encode each feature vector via dictionary using LLC.</li>
<li>In Spatial Pyramid Matching part, the pooled features from each sub-region are concatenated and normalized as the final image feature representation. In this paper, we use max pooling combined with sum normalization.</li>
</ol>

<h2>
<a id="dataset" class="anchor" href="#dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>DataSet</h2>

<p>The dataset we used is composed of fifteen scene categories by Svetlana Lazebnik et.al[1].  Each category has 200 to 400 images, and average image size is 300 Ã— 250 pixels. The major sources
of the pictures in the dataset include the COREL collection,
personal photographs, and Google image search. </p>

<h2>
<a id="evaluation" class="anchor" href="#evaluation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Evaluation</h2>

<p>When evaluate our model, we use a typical training/test split: 100 images per class for training and the rest for testing. </p>

<p>We report the confusion matrix for the fifteen scenes categories as well as the mean accuracy.</p>

<p>In the mean accuracy measure, we calculate the accuracy of each class, and average the accuracy values of all classes. </p>

<h2>
<a id="spatial-pyramid-matching-with-hard-coding" class="anchor" href="#spatial-pyramid-matching-with-hard-coding" aria-hidden="true"><span class="octicon octicon-link"></span></a>Spatial Pyramid Matching with Hard Coding</h2>

<p>After building dictionary, for each feature vector, we assign it to its closed cluster in dictionary. In other words, each feature vector is assigned to only one cluster. Note that in this experiment, different levels of features are concatenated with different weights. For the features in level k, its weight will be 1/2^(L-k), where L is the number of levels.</p>

<h3>
<a id="parameter-setting" class="anchor" href="#parameter-setting" aria-hidden="true"><span class="octicon octicon-link"></span></a>Parameter Setting</h3>

<pre><code>params.maxImageSize = 1000;
params.gridSpacing = 8;
params.patchSize = 16;
params.dictionarySize = 200;
params.numTextonImages = 100;
params.pyramidLevels = 3;
</code></pre>

<h3>
<a id="linear-kernel" class="anchor" href="#linear-kernel" aria-hidden="true"><span class="octicon octicon-link"></span></a>Linear Kernel</h3>

<p>We first built the model using linear kernel.  The features for each instance is hard-coded word. LibLinear[3] is used to train the model. The results we get is 0.7267.</p>

<p>Normalized Confusion Matrix</p>

<p>Note that the y-axis is the true label, the x-axis is the predicted label.</p>

<p><img src="https://github.com/wwu137/Locality-constrained-Linear-Coding-for-Scene-Classification/blob/master/result/nonLLC-3-5-200.jpg?raw=true" alt=""></p>

<p>Non-Normalized Confusion Matrix</p>

<p><img src="https://github.com/wwu137/Locality-constrained-Linear-Coding-for-Scene-Classification/blob/master/result/nonLLC-3-5-200-Raw.jpg?raw=true" alt=""></p>

<h3>
<a id="histogram-intersection-kernel" class="anchor" href="#histogram-intersection-kernel" aria-hidden="true"><span class="octicon octicon-link"></span></a>Histogram Intersection Kernel</h3>

<p>We also built the model using histogram intersection kernel. We first to calculate the kernel for each instance, then train the non-linear SVM model using our kernel. The results we get is 0.8664.</p>

<p>Normalized Confusion Matrix</p>

<p>Note that the y-axis is the true label, the x-axis is the predicted label.</p>

<p><img src="https://github.com/wwu137/Locality-constrained-Linear-Coding-for-Scene-Classification/blob/master/result/nonLLC_kernel-3-5-200.jpg?raw=true" alt=""></p>

<p>Non-Normalized Confusion Matrix</p>

<p><img src="https://github.com/wwu137/Locality-constrained-Linear-Coding-for-Scene-Classification/blob/master/result/nonLLC_kernel-3-5-200-Raw.jpg?raw=true" alt=""></p>

<h3>
<a id="summary" class="anchor" href="#summary" aria-hidden="true"><span class="octicon octicon-link"></span></a>Summary</h3>

<p>Based on the results, we can see using the histogram intersection kernel does improve our classification model. But due the computation for kernel, the computation complexity is higher than the linear SVM.</p>

<h2>
<a id="locality-constrained-linear-coding" class="anchor" href="#locality-constrained-linear-coding" aria-hidden="true"><span class="octicon octicon-link"></span></a>Locality-constrained Linear Coding</h2>

<p>LLC utilizes the locality constraints to project each descriptor into its local-coordinate system, and the projected coordinates are integrated by max pooling to generate the final representation.</p>

<h3>
<a id="parameter-setting-1" class="anchor" href="#parameter-setting-1" aria-hidden="true"><span class="octicon octicon-link"></span></a>Parameter Setting</h3>

<pre><code>params.maxImageSize = 1000;
params.gridSpacing = 8;
params.patchSize = 16;
params.dictionarySize = 2048;
params.numTextonImages = 100;
params.pyramidLevels = 3;
params.K = 5;
liblinear.solver = 3;
liblinear.C = 100;
liblinear.B = 1;
</code></pre>

<h3>
<a id="result" class="anchor" href="#result" aria-hidden="true"><span class="octicon octicon-link"></span></a>Result</h3>

<p>The mean accuracy we got is 81.14%.</p>

<p>Normalized Confusion Matrix</p>

<p>Note that the y-axis is the true label, the x-axis is the predicted label.</p>

<p><img src="https://github.com/wwu137/Locality-constrained-Linear-Coding-for-Scene-Classification/blob/master/result/cmat_LLC_norm_K5_dic2048_l3_s3_C100_B1.jpg?raw=true" alt="Confusion Matrix"></p>

<p>Non-Normalized Confusion Matrix</p>

<p><img src="https://github.com/wwu137/Locality-constrained-Linear-Coding-for-Scene-Classification/blob/master/result/cmat_LLC_norm_K5_dic2048_l3_s3_C100_B1.jpg?raw=true" alt=""></p>

<h3>
<a id="grid-searchbonus" class="anchor" href="#grid-searchbonus" aria-hidden="true"><span class="octicon octicon-link"></span></a>Grid Search(Bonus)</h3>

<p>We did extensive grid search for LLC model. The parameters we consider is Solver Type provided by liblinear, Cost for SVM, include bias or not, and dictionary size.</p>

<p>The solver types we consider are list as below:</p>

<table>
<thead>
<tr>
<th>Solver</th>
<th>Index</th>
</tr>
</thead>
<tbody>
<tr>
<td>L2-regularized logistic regression (primal)</td>
<td>0</td>
</tr>
<tr>
<td>L2-regularized L2-loss support vector classification (dual)</td>
<td>1</td>
</tr>
<tr>
<td>L2-regularized L2-loss support vector classification (primal)</td>
<td>2</td>
</tr>
<tr>
<td>L2-regularized L1-loss support vector classification (dual)</td>
<td>3</td>
</tr>
<tr>
<td>support vector classification by Crammer and Singer</td>
<td>4</td>
</tr>
<tr>
<td>L1-regularized L2-loss support vector classification</td>
<td>5</td>
</tr>
</tbody>
</table>

<p>For the cost, we consider five different values: 0.01,0.1,1,10,100.</p>

<p>We also consider whether to include the bias or not in the model.</p>

<p>Finally, we consider the dictionary size as 1024 and 2048.</p>

<table>
<thead>
<tr>
<th>Dictionary Size</th>
<th>Cost</th>
<th>Bias</th>
<th>Solver</th>
<th>Mean Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>1024</td>
<td>0.01</td>
<td>-1</td>
<td>0</td>
<td>61.95</td>
</tr>
<tr>
<td>1024</td>
<td>0.1</td>
<td>-1</td>
<td>0</td>
<td>66.95</td>
</tr>
<tr>
<td>1024</td>
<td>1</td>
<td>-1</td>
<td>0</td>
<td>75.39</td>
</tr>
<tr>
<td>1024</td>
<td>10</td>
<td>-1</td>
<td>0</td>
<td>79.54</td>
</tr>
<tr>
<td>1024</td>
<td>100</td>
<td>-1</td>
<td>0</td>
<td>79.8</td>
</tr>
<tr>
<td>1024</td>
<td>0.01</td>
<td>-1</td>
<td>1</td>
<td>66.35</td>
</tr>
<tr>
<td>1024</td>
<td>0.1</td>
<td>-1</td>
<td>1</td>
<td>74.67</td>
</tr>
<tr>
<td>1024</td>
<td>1</td>
<td>-1</td>
<td>1</td>
<td>79.07</td>
</tr>
<tr>
<td>1024</td>
<td>10</td>
<td>-1</td>
<td>1</td>
<td>79.2</td>
</tr>
<tr>
<td>1024</td>
<td>100</td>
<td>-1</td>
<td>1</td>
<td>79.08</td>
</tr>
<tr>
<td>1024</td>
<td>0.01</td>
<td>-1</td>
<td>2</td>
<td>66.39</td>
</tr>
<tr>
<td>1024</td>
<td>0.1</td>
<td>-1</td>
<td>2</td>
<td>74.67</td>
</tr>
<tr>
<td>1024</td>
<td>1</td>
<td>-1</td>
<td>2</td>
<td>79.02</td>
</tr>
<tr>
<td>1024</td>
<td>10</td>
<td>-1</td>
<td>2</td>
<td>79.1</td>
</tr>
<tr>
<td>1024</td>
<td>100</td>
<td>-1</td>
<td>2</td>
<td>79.21</td>
</tr>
<tr>
<td>1024</td>
<td>0.01</td>
<td>-1</td>
<td>3</td>
<td>54.81</td>
</tr>
<tr>
<td>1024</td>
<td>0.1</td>
<td>-1</td>
<td>3</td>
<td>69.04</td>
</tr>
<tr>
<td>1024</td>
<td>1</td>
<td>-1</td>
<td>3</td>
<td>75.37</td>
</tr>
<tr>
<td>1024</td>
<td>10</td>
<td>-1</td>
<td>3</td>
<td>79.02</td>
</tr>
<tr>
<td>1024</td>
<td>100</td>
<td>-1</td>
<td>3</td>
<td>79.02</td>
</tr>
<tr>
<td>1024</td>
<td>0.01</td>
<td>-1</td>
<td>4</td>
<td>70.7</td>
</tr>
<tr>
<td>1024</td>
<td>0.1</td>
<td>-1</td>
<td>4</td>
<td>71</td>
</tr>
<tr>
<td>1024</td>
<td>1</td>
<td>-1</td>
<td>4</td>
<td>79.04</td>
</tr>
<tr>
<td>1024</td>
<td>10</td>
<td>-1</td>
<td>4</td>
<td>79.88</td>
</tr>
<tr>
<td>1024</td>
<td>100</td>
<td>-1</td>
<td>4</td>
<td>79.88</td>
</tr>
<tr>
<td>1024</td>
<td>0.01</td>
<td>-1</td>
<td>5</td>
<td>6.67</td>
</tr>
<tr>
<td>1024</td>
<td>0.1</td>
<td>-1</td>
<td>5</td>
<td>42.12</td>
</tr>
<tr>
<td>1024</td>
<td>1</td>
<td>-1</td>
<td>5</td>
<td>63.61</td>
</tr>
<tr>
<td>1024</td>
<td>10</td>
<td>-1</td>
<td>5</td>
<td>68.71</td>
</tr>
<tr>
<td>1024</td>
<td>100</td>
<td>-1</td>
<td>5</td>
<td>68.21</td>
</tr>
<tr>
<td>1024</td>
<td>0.01</td>
<td>1</td>
<td>0</td>
<td>61.7</td>
</tr>
<tr>
<td>1024</td>
<td>0.1</td>
<td>1</td>
<td>0</td>
<td>66.31</td>
</tr>
<tr>
<td>1024</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>75.44</td>
</tr>
<tr>
<td>1024</td>
<td>10</td>
<td>1</td>
<td>0</td>
<td>79.3</td>
</tr>
<tr>
<td>1024</td>
<td>100</td>
<td>1</td>
<td>0</td>
<td>79.92</td>
</tr>
<tr>
<td>1024</td>
<td>0.01</td>
<td>1</td>
<td>1</td>
<td>65.95</td>
</tr>
<tr>
<td>1024</td>
<td>0.1</td>
<td>1</td>
<td>1</td>
<td>74.91</td>
</tr>
<tr>
<td>1024</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>79.54</td>
</tr>
<tr>
<td>1024</td>
<td>10</td>
<td>1</td>
<td>1</td>
<td>79.73</td>
</tr>
<tr>
<td>1024</td>
<td>100</td>
<td>1</td>
<td>1</td>
<td>79.73</td>
</tr>
<tr>
<td>1024</td>
<td>0.01</td>
<td>1</td>
<td>2</td>
<td>66.04</td>
</tr>
<tr>
<td>1024</td>
<td>0.1</td>
<td>1</td>
<td>2</td>
<td>74.88</td>
</tr>
<tr>
<td>1024</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>79.68</td>
</tr>
<tr>
<td>1024</td>
<td>10</td>
<td>1</td>
<td>2</td>
<td>79.63</td>
</tr>
<tr>
<td>1024</td>
<td>100</td>
<td>1</td>
<td>2</td>
<td>79.57</td>
</tr>
<tr>
<td>1024</td>
<td>0.01</td>
<td>1</td>
<td>3</td>
<td>51</td>
</tr>
<tr>
<td>1024</td>
<td>0.1</td>
<td>1</td>
<td>3</td>
<td>70.97</td>
</tr>
<tr>
<td>1024</td>
<td>1</td>
<td>1</td>
<td>3</td>
<td>75.46</td>
</tr>
<tr>
<td>1024</td>
<td>10</td>
<td>1</td>
<td>3</td>
<td>79.72</td>
</tr>
<tr>
<td>1024</td>
<td>100</td>
<td>1</td>
<td>3</td>
<td>79.72</td>
</tr>
<tr>
<td>1024</td>
<td>0.01</td>
<td>1</td>
<td>4</td>
<td>64.11</td>
</tr>
<tr>
<td>1024</td>
<td>0.1</td>
<td>1</td>
<td>4</td>
<td>71.3</td>
</tr>
<tr>
<td>1024</td>
<td>1</td>
<td>1</td>
<td>4</td>
<td>79.98</td>
</tr>
<tr>
<td>1024</td>
<td>10</td>
<td>1</td>
<td>4</td>
<td>80.12</td>
</tr>
<tr>
<td>1024</td>
<td>100</td>
<td>1</td>
<td>4</td>
<td>80.12</td>
</tr>
<tr>
<td>1024</td>
<td>0.01</td>
<td>1</td>
<td>5</td>
<td>6.67</td>
</tr>
<tr>
<td>1024</td>
<td>0.1</td>
<td>1</td>
<td>5</td>
<td>10.86</td>
</tr>
<tr>
<td>1024</td>
<td>1</td>
<td>1</td>
<td>5</td>
<td>62.6</td>
</tr>
<tr>
<td>1024</td>
<td>10</td>
<td>1</td>
<td>5</td>
<td>68.55</td>
</tr>
<tr>
<td>1024</td>
<td>100</td>
<td>1</td>
<td>5</td>
<td>67.02</td>
</tr>
<tr>
<td>2048</td>
<td>0.01</td>
<td>-1</td>
<td>0</td>
<td>61.91</td>
</tr>
<tr>
<td>2048</td>
<td>0.1</td>
<td>-1</td>
<td>0</td>
<td>67.02</td>
</tr>
<tr>
<td>2048</td>
<td>1</td>
<td>-1</td>
<td>0</td>
<td>75.55</td>
</tr>
<tr>
<td>2048</td>
<td>10</td>
<td>-1</td>
<td>0</td>
<td>79.48</td>
</tr>
<tr>
<td>2048</td>
<td>100</td>
<td>-1</td>
<td>0</td>
<td>80.1</td>
</tr>
<tr>
<td>2048</td>
<td>0.01</td>
<td>-1</td>
<td>1</td>
<td>66.89</td>
</tr>
<tr>
<td>2048</td>
<td>0.1</td>
<td>-1</td>
<td>1</td>
<td>75.25</td>
</tr>
<tr>
<td>2048</td>
<td>1</td>
<td>-1</td>
<td>1</td>
<td>79.64</td>
</tr>
<tr>
<td>2048</td>
<td>10</td>
<td>-1</td>
<td>1</td>
<td>80</td>
</tr>
<tr>
<td>2048</td>
<td>100</td>
<td>-1</td>
<td>1</td>
<td>80.14</td>
</tr>
<tr>
<td>2048</td>
<td>0.01</td>
<td>-1</td>
<td>2</td>
<td>66.85</td>
</tr>
<tr>
<td>2048</td>
<td>0.1</td>
<td>-1</td>
<td>2</td>
<td>75.18</td>
</tr>
<tr>
<td>2048</td>
<td>1</td>
<td>-1</td>
<td>2</td>
<td>79.63</td>
</tr>
<tr>
<td>2048</td>
<td>10</td>
<td>-1</td>
<td>2</td>
<td>80.2</td>
</tr>
<tr>
<td>2048</td>
<td>100</td>
<td>-1</td>
<td>2</td>
<td>80.25</td>
</tr>
<tr>
<td>2048</td>
<td>0.01</td>
<td>-1</td>
<td>3</td>
<td>55.27</td>
</tr>
<tr>
<td>2048</td>
<td>0.1</td>
<td>-1</td>
<td>3</td>
<td>68.96</td>
</tr>
<tr>
<td>2048</td>
<td>1</td>
<td>-1</td>
<td>3</td>
<td>76.4</td>
</tr>
<tr>
<td>2048</td>
<td>10</td>
<td>-1</td>
<td>3</td>
<td>80.05</td>
</tr>
<tr>
<td>2048</td>
<td>100</td>
<td>-1</td>
<td>3</td>
<td>80.05</td>
</tr>
<tr>
<td>2048</td>
<td>0.01</td>
<td>-1</td>
<td>4</td>
<td>70.64</td>
</tr>
<tr>
<td>2048</td>
<td>0.1</td>
<td>-1</td>
<td>4</td>
<td>70.75</td>
</tr>
<tr>
<td>2048</td>
<td>1</td>
<td>-1</td>
<td>4</td>
<td>80.3</td>
</tr>
<tr>
<td>2048</td>
<td>10</td>
<td>-1</td>
<td>4</td>
<td>80.71</td>
</tr>
<tr>
<td>2048</td>
<td>100</td>
<td>-1</td>
<td>4</td>
<td>80.71</td>
</tr>
<tr>
<td>2048</td>
<td>0.01</td>
<td>-1</td>
<td>5</td>
<td>6.67</td>
</tr>
<tr>
<td>2048</td>
<td>0.1</td>
<td>-1</td>
<td>5</td>
<td>47.77</td>
</tr>
<tr>
<td>2048</td>
<td>1</td>
<td>-1</td>
<td>5</td>
<td>62.87</td>
</tr>
<tr>
<td>2048</td>
<td>10</td>
<td>-1</td>
<td>5</td>
<td>68.19</td>
</tr>
<tr>
<td>2048</td>
<td>100</td>
<td>-1</td>
<td>5</td>
<td>67.34</td>
</tr>
<tr>
<td>2048</td>
<td>0.01</td>
<td>1</td>
<td>0</td>
<td>61.92</td>
</tr>
<tr>
<td>2048</td>
<td>0.1</td>
<td>1</td>
<td>0</td>
<td>66.56</td>
</tr>
<tr>
<td>2048</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>75.47</td>
</tr>
<tr>
<td>2048</td>
<td>10</td>
<td>1</td>
<td>0</td>
<td>79.71</td>
</tr>
<tr>
<td>2048</td>
<td>100</td>
<td>1</td>
<td>0</td>
<td>80.58</td>
</tr>
<tr>
<td>2048</td>
<td>0.01</td>
<td>1</td>
<td>1</td>
<td>66.19</td>
</tr>
<tr>
<td>2048</td>
<td>0.1</td>
<td>1</td>
<td>1</td>
<td>75.15</td>
</tr>
<tr>
<td>2048</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>80.13</td>
</tr>
<tr>
<td>2048</td>
<td>10</td>
<td>1</td>
<td>1</td>
<td>80.86</td>
</tr>
<tr>
<td>2048</td>
<td>100</td>
<td>1</td>
<td>1</td>
<td>80.87</td>
</tr>
<tr>
<td>2048</td>
<td>0.01</td>
<td>1</td>
<td>2</td>
<td>66.3</td>
</tr>
<tr>
<td>2048</td>
<td>0.1</td>
<td>1</td>
<td>2</td>
<td>75.22</td>
</tr>
<tr>
<td>2048</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>80.04</td>
</tr>
<tr>
<td>2048</td>
<td>10</td>
<td>1</td>
<td>2</td>
<td>80.8</td>
</tr>
<tr>
<td>2048</td>
<td>100</td>
<td>1</td>
<td>2</td>
<td>80.42</td>
</tr>
<tr>
<td>2048</td>
<td>0.01</td>
<td>1</td>
<td>3</td>
<td>57.78</td>
</tr>
<tr>
<td>2048</td>
<td>0.1</td>
<td>1</td>
<td>3</td>
<td>71.06</td>
</tr>
<tr>
<td>2048</td>
<td>1</td>
<td>1</td>
<td>3</td>
<td>76.84</td>
</tr>
<tr>
<td>2048</td>
<td>10</td>
<td>1</td>
<td>3</td>
<td>81.14</td>
</tr>
<tr>
<td>2048</td>
<td>100</td>
<td>1</td>
<td>3</td>
<td>81.14</td>
</tr>
<tr>
<td>2048</td>
<td>0.01</td>
<td>1</td>
<td>4</td>
<td>68.91</td>
</tr>
<tr>
<td>2048</td>
<td>0.1</td>
<td>1</td>
<td>4</td>
<td>70.15</td>
</tr>
<tr>
<td>2048</td>
<td>1</td>
<td>1</td>
<td>4</td>
<td>80.27</td>
</tr>
<tr>
<td>2048</td>
<td>10</td>
<td>1</td>
<td>4</td>
<td>80.66</td>
</tr>
<tr>
<td>2048</td>
<td>100</td>
<td>1</td>
<td>4</td>
<td>80.66</td>
</tr>
<tr>
<td>2048</td>
<td>0.01</td>
<td>1</td>
<td>5</td>
<td>6.67</td>
</tr>
<tr>
<td>2048</td>
<td>0.1</td>
<td>1</td>
<td>5</td>
<td>6.67</td>
</tr>
<tr>
<td>2048</td>
<td>1</td>
<td>1</td>
<td>5</td>
<td>60.59</td>
</tr>
<tr>
<td>2048</td>
<td>10</td>
<td>1</td>
<td>5</td>
<td>68.96</td>
</tr>
<tr>
<td>2048</td>
<td>100</td>
<td>1</td>
<td>5</td>
<td>66.63</td>
</tr>
</tbody>
</table>

<h2>
<a id="reference" class="anchor" href="#reference" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reference</h2>

<ol>
<li>Lazebnik, S., Schmid, C., &amp; Ponce, J. (2006). <em>Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</em>. In Computer Vision and Pattern Recognition, 2006 IEEE Computer Society Conference on (Vol. 2, pp. 2169-2178). IEEE.</li>
<li>Wang, J., Yang, J., Yu, K., Lv, F., Huang, T., &amp; Gong, Y. (2010, June). <em>Locality-constrained linear coding for image classification</em>. In Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on (pp. 3360-3367). IEEE.</li>
<li>Fan, R. E., Chang, K. W., Hsieh, C. J., Wang, X. R., &amp; Lin, C. J. (2008). <em>LIBLINEAR: A library for large linear classification</em>. The Journal of Machine Learning Research, 9, 1871-1874.</li>
</ol>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Locality-constrained-linear-coding-for-scene-classification maintained by <a href="https://github.com/wwu137">wwu137</a></p>
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
